\documentclass{article}
\setlength{\parindent}{0ex}
\setlength{\parskip}{1em}
\usepackage[utf8]{inputenc} 
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{fancybox}
\usepackage{tikz}
\usepackage{tkz-euclide}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{qtree}
\usepackage{scrextend}
\usepackage{multirow}
\usepackage{float}
\usepackage{algpseudocode}


\tikzset{main node/.style={circle,fill=blue!20,draw,minimum size=1cm,inner sep=0pt},
}


%Kodestyling \begin{lstlisting}
\usepackage{color}
\usepackage{listings}
\lstset{ %
language=C++,                % choose the language of the code
%basicstyle=\footnotesize,       % the size of the fonts that are used for the code
basicstyle=\ttfamily,
%numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it is 1 each line will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
%frame=single,           % adds a frame around the code
tabsize=2,          % sets default tabsize to 2 spaces
captionpos=b,           % sets the caption-position to bottom
breaklines=true,        % sets automatic line breaking
breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)},          % if you want to add a comment within your code
mathescape
}

\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}


\def\meta#1{\mbox{$\langle\hbox{#1}\rangle$}}
\def\macrowitharg#1#2{{\tt\string#1\bra\meta{#2}\ket}}

{\escapechar-1 \xdef\bra{\string\{}\xdef\ket{\string\}}}

\def\intro#1{{#1}{\cal I}}
\def\elim#1{{#1}{\cal E}}

\showboxbreadth 999
\showboxdepth 999
\tracingoutput 1


\let\imp\to
\def\elim#1{{{#1}{\cal E}}}
\def\intro#1{{{#1}{\cal I}}}
\def\lt{<}
\def\eqdef{=}
\def\eps{\mathrel{\epsilon}}
\def\biimplies{\leftrightarrow}
\def\flt#1{\mathrel{{#1}^\flat}}
\def\setof#1{{\left\{{#1}\right\}}}
\let\implies\to
\def\KK{{\mathsf K}}
\let\squashmuskip\relax

\graphicspath{ {images/} }
\usetikzlibrary{arrows}
\tikzset{
  leaf_/.style = {shape=rectangle,draw, align=center},
  node_/.style     = {shape=circle,draw,align=center}
}
\author{Rune Kok Nielsen (qkd362), Andreas Holm (jnh508)}
\title{Afløsningsopgave i fagområdet Modellering og Analyse af Data}
\DeclareMathOperator{\Ran}{Ran}
\DeclareMathOperator{\Dom}{Dom}

\renewcommand*\contentsname{Indholdsfortegnelse}
\begin{document}

\maketitle

\newpage
\tableofcontents
\newpage

\section{Introduktion}
Denne rapport omhandler klassificering af grafer gennem \textit{support vector machines} (SVM) motiveret af et abstract \cite{trivial-kernels}, som udfordrer etablerede grafkernemetoder ved at foreslå en række simplere metoder, der ifølge deres resultater er tilstrækkelige. Da rapporten er en afløsningsopgave i fagområdet \textit{Modellering og Analyse af Data}, vil vi først beskrive den grundlæggende teori i klassificering og SVM. Herefter fokuserer vi på klassificering af grafer gennem SVM ved at beskrive nogle eksempler på grafkerner. Til sidst gentager vi eksperimenterne fra \cite{trivial-kernels} og sammenligner resultaterne.


\section{Klassificering}
Formålet med klassificering er at putte datapunkter bestående af et antal egenskaber i passende kategorier fra en prædifineret mængde af diskrete, beskrivende kategorier. Disse datapunkter er også kendt som tupler bestående af $(x,y)$, hvor $x$ er de givne egenskaber, og $y$ er punktets kategori. Klassificering består i definitionen af at finde en passende klassificeringsmodel, hvilket er en funktion $f$ der tager et datapunkt $x$ og returnerer den tilhørende kategori $y$.

Der findes adskillige teknikker til at bestemme en klassificeringsmodel. En sådan teknik beskriver en læringsalgoritme, der på basis af et træningssæt (et sæt af datapunkter med kendte kategorier) udleder en (evt. suboptimal) model.

Det er sjældent muligt at klassificere ethvert givent punkt korrekt ud fra de tilgængelige egenskaber, og vi har derfor brug for en måde at afgøre nøjagtigheden af den resulterende model. Denne nøjagtighed estimeres ved at klassificere et testsæt med modellen, og se på hvor stor en andel af datapunkterne der har fået den rigtige kategori. For at vise modellens nøjagtighed kan man beregne dens \textit{accuracy}, hvilket fortæller hvor præcis den er, eller man kan beregne dens \textit{error rate}, hvilke er det modsatte af \textit{accuracy} dvs. hvor mange forkerte punkter modellen har produceret.\\
For at beregne nøjagtigheden af en model, benytter vi et testsæt. Ligesom træningssættet er testsættet en mængde af datapunkter med kendte kategorier. Udregningerne består da i at klassificere punkterne i testsættet ved hjælp af modellen, og sammenligne punkternes kategorier som udregnet af modellen med deres egentlige kategorier, som vi kender i forvejen.\\
Det er essentielt at testsættet ikke overlapper med træningssættet, da dette vil tildele en for høj nøjagtighed til modellen. Har man f.eks.\ et meget lille træningssæt kan man nemt lave en model, som tilfældigvis passer perfekt på træningssættet, men som i realiteten er helt forkert. Tester man modellen på et testsæt som næsten overlapper fuldstændigt med træningssættet vil de overlappende punkter højst sandsynligt klassificeres korrekt, hvilket resulterer i meget høj nøjagtighed, selvom modellen i virkeligheden kun passer netop på træningssættet. 

For at udregne nøjagtiheden og fejlraten af klassificeringen bruger vi en \textit{confusion matrix}. En \textit{confusion matrix} ser formelt ud som i tabel 1 nedenfor. Her angiver $f_{ij}$ antallet af datapunkter i testsættet som tilhører klasse $i$, og blev klassificeret som klasse $j$. I det optimale tilfælde, hvor alle datapunkter klassificeres korrekt, har vi intuitivt $f_{ij}=0$ for $i\neq j$, hvilket manifesteres i en diagonal matrice.\\

\begin{table}[H]
\begin{center}
\begin{tabular}{cc|c|c|c|c|}
    & \multicolumn{5}{c}{Predicted Class} \\
    \cline{3-6}
     & & Class = 0 & Class = 1 & .. & Class = n \\
    \cline{2-6}
    \multicolumn{1}{c|}{\multirow{4}{*}{Actual Class}} & Class = 0 & $f_{00}$ & $f_{01}$ & .. & $f_{0n}$ \\
    \cline{2-6}
    \multicolumn{1}{c|}{} & Class = 1 & $f_{10}$ & $f_{11}$ & .. & $f_{1n}$ \\
    \cline{2-6}
    \multicolumn{1}{c|}{} & .. & .. & .. & .. & .. \\
    \cline{2-6}
    \multicolumn{1}{c|}{} & Class = n & $f_{n0}$ & $f_{n1}$ & .. & $f_{nn}$ \\
    \cline{2-6}
\end{tabular}
\caption{\textit{Confusion matrix}}
\end{center}
\end{table}

For at beregne vores \textit{accuracy} og \textit{error rate} kan vi finde antallet af korrekte- og forkerte forudsigelser ved at summere tallene på hhv. uden for diagonalen, og dividere værdierne med det totale antal klassificeringer. Vi får:\\

\begin{align*}
Accuracy &= \frac{Antallet \ af \ korrekte \ forudsigelser}{Det \ totale \ antal \ forudsigelser} \\
         &= \frac{\sum_{i=1}^{n}f_{ii}}{\sum_{i=1}^{n}\sum_{j=1}^{n}f_{ij}}
\end{align*}

\begin{align*}
Error \ rate &= \frac{Antallet \ af \ forkerte \ forudsigelser}{Det \ totale \ antal \ forudsigelser} \\
           &= \frac{\sum_{i=1}^{n}\sum_{j=1}^{n}e(i,j)}{\sum_{i=1}^{n}\sum_{j=1}^{n}f_{ij}}\\
           \text{hvor }e(i,j)&=
           \begin{cases} f_{ij} & \text{for } i\neq j \\
           0                                    & \text{ellers}      %
           \end{cases}
\end{align*}


Her gælder det klart at jo højere \textit{accuracy} desto bedre, og det modsatte gælder for \textit{error rate}. \\

\subsubsection{Problemer ved klassificering}
Det er ikke altid muligt at lave en perfekt klassificeringsmodel, som klassificerer ethvert givent punkt korrekt. F.eks. kunne datæn være utilstrækkelig til, at ramme alle tænkelige eksempler korrekt. Betragt træningssættet nedenfor. 

\begin{tabular}{c|c|c|c}
	Dyr & Lægger æg & Har et næb & Klasse\\
	\hline
	Pingvin & Ja & Ja & Fugl\\
	Aborre & Ja & Nej & Fisk\\
	Gorilla & Nej & Nej & Pattedyr\\
	Torsk & Ja & Nej & Fisk\\
	Due & Ja & Ja & Fugl\\
	Kat & Nej & Nej & Pattedyr
\end{tabular}

Datasættet indeholder kun to binære egenskaber, men ud fra træningssættet kan man nemt lave en model, som lader til at tildelte ethvert dyr i korrekt kategori (antaget at det ligger inde for en af de tre mulige klasser). Nedenfor ses et beslutningstræ, som klassificerer alle punkterne i træningssættet korrekt (mere om beslutingstræer i næste afsnit).

\begin{center}
	\begin{tikzpicture}
	[
	grow                    = down,
	sibling distance        = 6em,
	level distance          = 6em,
	edge from parent/.style = {draw, -latex},
	every node/.style       = {font=\footnotesize}
	]
	\node [node_] {Lægger æg?}
	child { node [leaf_] {Pattedyr}
		edge from parent node [left] {Nej} }
	child { node [node_] {Har et næb?}
		child { node [leaf_] {Fisk}
			edge from parent node [left]{Nej} }       
		child { node [leaf_] {Fugl}
			edge from parent node [right]{Ja} }
		edge from parent node [right] {Ja} };
	\end{tikzpicture}
\end{center}
Dette ser undertiden fint ud, men prøver man sætte et næbdyr ind i denne model, vil den blive klassificeret som en fugl (da den har et næb og lægger æg), selvom den egentlig er et pattedyr. Selvom vi egentlig har en model, som måske kan klassificere de fleste dyr korrekt, vil der være specielle tilfælde, hvor den skyder helt ved siden af. I dette eksempel kunne man muligvis løse problemet ved at indsamle flere egenskaber, og benytte et større træningssæt, men dette vil ikke altid være muligt, og der vil som sådan være situationer, hvor man ikke kan bygge en perfekt klassificeringsmodel.


\subsection{Beslutningstræer (Decision trees)}
Et eksempel på en klassificeringsteknik er gennem brug af beslutningstræer. Et beslutningstræ består af knuder og blade samt en rodknude. Et datapunkt kan da klassificeres ved at traversere træet startende i roden.

Hver knude består af et spørgsmål, som har en definitiv mængde svar, der kan besvares ud fra egenskaberne i et givent datapunkt. Ud fra svaret (altså datapunktets egenskaber) peger knuden videre til en ny knude eller et blad. Et blad repræsenterer den endelige kategori. I eksemplet i figur 1, kan der ses et eksempel på hvordan et beslutningstræ fungerer. I dette eksempel antager vi, at vi har fået en mængde, som indeholder enten biler, fly eller både. Vi starter med at spørge om det givet datapunkt har hjul, hvis nej må det være en båd, hvis ja kan det enten være en bil eller et fly, og derfor spørger vi igen, denne gang om det givet datapunkt har vinger, hvis ja er det et fly, og hvis nej må det være en bil. På denne måde har vi sikret os at vores datapunkt er kommet i den rigtige kategori.

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
  [
    grow                    = down,
    sibling distance        = 6em,
    level distance          = 6em,
    edge from parent/.style = {draw, -latex},
    every node/.style       = {font=\footnotesize}
  ]
  \node [node_] {Hjul?}
    child { node [leaf_] {Båd}
      edge from parent node [left] {Nej} }
    child { node [node_] {Vinger?}
      child { node [leaf_] {Bil}
              edge from parent node [left]{Nej} }       
      child { node [leaf_] {Fly}
              edge from parent node [right]{Ja} }
              edge from parent node [right] {Ja} };
\end{tikzpicture}
\end{center}
\caption{Beslutningstræ}
\end{figure}

Et (muligvis suboptimalt) beslutningstræ kan konstrueres fra et træningssæt med Hunts algoritme. Algoritmen starter med en mængde af alle datapunkter i træningssættet. Hvis alle punkter i mængden har samme kategori, bliver denne mængde et blad i træet. Ellers stilles et spørgsmål om punkterne i mængden, og disse punkter samles i delmængder ud fra, hvordan de besvarer spørgsmålet. Spørgsmålet bliver en knude i træet, som peger på de nye delmængder, og algoritmen køres rekursivt på delmængderne. Effektiviteten og præcisionen af en sådan algoritme afhænger dermed af, at den kan stille de rigtige spørgsmål.


\subsubsection{Eksempel på modeludledning}
For at illustrere brugen af decision trees vil vi her vise et eksempel på, hvordan man ud fra et todimensionelt træningssæt og testsæt kan bygge og efterteste en klassificeringsmodel. Betragt træningssættet til venstre i figur 2 nedenfor. Tegner vi punkterne ind i et koordinatsystem opnår vi figuren til højre, hvor blå cirkler er A, røde plusser er B og gule stjerner er C.

\begin{figure}[H]
	\begin{minipage}[t]{0.27\linewidth}
		\begin{flushleft}
			\begin{tabular}{c|c|c}
				X & Y & Klasse\\
				\hline
				0.8 & 0.9 & A\\
				0.6 & 0.1 & B\\
				0.2 & 0.3 & C\\
				0.1 & 0.8 & C\\
				0.4 & 0.2 & B\\
				0.4 & 0.5 & A\\
				0.03 & 0.6 & C\\
				0.5 & 0. 3 & B\\
				0.6 & 0.7 & A\\
				0.45 & 0.8 & A\\
				0.8 & 0.7 & B\\
				0.28 & 0.3 & C\\
				0.1 & 0.05 & C
			\end{tabular}
		\end{flushleft}
	\end{minipage}
	\begin{minipage}[t]{0.25\linewidth}
		\begin{flushright}
			\includegraphics[width=9cm]{decision_tree_example_plot}
		\end{flushright}
	\end{minipage}
	\caption{\textit{Til venstre: eksempel på træningssæt. Til højre: plot af træningssættet.}}
\end{figure}
\begin{figure}[H]
	\begin{minipage}[t]{0.5\linewidth}
		\begin{flushleft}
			\ \\ \ \\
			Der er klare tendenser i, hvordan punkterne har fordelt sig. Ved at indtegne et par linjer kan vi tydeligt illustrere dette (figur 3).
		\end{flushleft}
	\end{minipage}
	\begin{minipage}[t]{0.25\linewidth}
		\begin{flushright}
			\includegraphics[width=8cm]{decision_tree_example_plot_splitted}
		\end{flushright}
	\end{minipage}
	\caption{\textit{Til højre: opdelt plot (se figur 2)}}
\end{figure}



Ud fra denne opdeling kunne det tyde på, at $f(x,y)=C$ for $x<0.3$, $f(x,y)=A$ for $0.3<x<y$ og $f(x,y)=B$ for $0.3<x$ og $y<x$. Med disse observationer kan vi bygge et beslutningstræ med Hunts algoritme.

Lad $D_t$ være hele træningssættet. Så indeholder $D_t$ punkter af alle klasser. Vi vælger nu en (\textit{attribute test condition}) at adskille punkterne på. For at få det mindste træ, starter vi med at teste om $x<0.3$, hvilket giver os en mængde $D_{t1}=\{p\ |\ class(p)=C\}$ for $0.3<x$ og $D_{t2}=\{p\ |\ class(p)\neq C\}$ for $x<0.3$. Da $D_{t1}$ kun indeholder en klasse bliver dette til et $C-$blad i træet, mens vi må teste rekursivt på $D_{t2}$.

Vi tester $D_{t2}$ for egenskaben $x<y$ og får en gruppe $D_{t3}=\{p\ | class(p)=A\}$ for $x<y$ og $D_{t4}=\{p\ |\ class(p)=B\}$ for $y<x$. Da begge disse grupper er rene (kun indeholder punkter af samme klasse) bliver disse til blade, og vi kan nu tegne vores beslutningstræ.

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
		[
		grow                    = down,
		sibling distance        = 6em,
		level distance          = 6em,
		edge from parent/.style = {draw, -latex},
		every node/.style       = {font=\footnotesize}
		]
		\node [node_] {$x<0.3$}
		child { node [leaf_] {C}
			edge from parent node [left] {Ja} }
		child { node [node_] {$x<y$}
			child { node [leaf_] {A}
				edge from parent node [left]{Ja} }       
			child { node [leaf_] {B}
				edge from parent node [right]{Nej} }
			edge from parent node [right] {Nej} };
		\end{tikzpicture}
	\end{center}
	\caption{\textit{Endeligt beslutningstræ}}
\end{figure}

Det er nu lykkedes at bygge en model, som kan opdele træningssættet korrekt. Dette er dog ingen garanti for, at modellen er korrekt. Modellen er bygget ud fra nogle antagelser, som holder gennem træningssættet, men det betyder ikke, at det holder i praksis. For at undersøge modellens nøjagtighed, vil vi bruge den til at klassificere et testsæt. I figur 5 har vi plottet et testsæt med opdelingen fra tidligere.

\begin{figure}[H]
	\begin{minipage}[t]{0.27\linewidth}
		\begin{flushleft}
			\begin{tabular}{c|c|c}
				X & Y & Klasse\\
				\hline
				0.2 & 0.01 & C\\
				0.28 & 0.1 & C\\
				0.1 & 0.2 & C\\
				0.4 & 0.5 & A\\
				0.5 & 0.8 & A\\
				0.9 & 0.95 & A\\
				0.5 & 0.4 & B\\
				0.8 & 0.2 & B\\
				0.6 & 0.5 & A\\
				0.7 & 0.1 & B\\
				0.5 & 0.12 & B\\
				0.12 & 0.75 & C
			\end{tabular} 
		\end{flushleft}
	\end{minipage}
	\begin{minipage}[t]{0.1\linewidth}
		\begin{flushright}
			\includegraphics[width=9cm]{decision_tree_example_plot_test}
		\end{flushright}
	\end{minipage}
	\caption{\textit{Til venstre: Testsæt Til højre: Plot af testsæt}}
\end{figure}

Testsættet passer næsten overens, men der er et enkelt punkt som ikke overholder vores antagelse, så modellen er ikke perfekt. Helt konkret kan vi udregne en nøjagtighed baseret på vores resultat med testsættet. Ud af 12 punkter blev 11 punkter klassificeret korrekt, så vi regner os frem til en nøjagtihed $acc_M=\frac{11}{12}\approx0.92$


\section{Support Vector Machines (SVM)}

En SVM træningsalgoritme er en type klassifikationsteknik til binær klassifikation, som er specielt egnet til kontinuert data af mange dimensioner. Teknikken baseres på at finde et \textit{maximal margin hyperplane}, altså et hyperplan som deler datapunkterne op i to dele svarende til deres kategori, med maksimal margin til de nærmeste punkter. Dette hyperplan betegnes som modellens \textit{decision boundary}.

Formålet med at maksimere margin er at optimere modellens generaliseringsevne, og dermed opnå bedst mulig nøjagtighed, når nye punkter skal klassificeres. Hvis vores decision boundary har meget lille margin, er der intuitivt ikke plads til meget forskel på dataen sammenlignet med træningssættet. Dette koncept illustreres i eksemplet nedenfor.

Betragt træningssættet af todimensionelle punkter, markeret som plusser og cirkler afhængig af deres klasse (figur 6 til venstre). På figur 6 til højre er indtegnet to hyperplaner (sorte linjer) og deres margin (grå linjer), som begge deler træningssættet op uden fejl. Det er tydeligt at B2 har bredere margin end B1, og B2 er derfor objektivt den bedste løsning til problemet. Undersøger man selv dataen for en naturlig tendens i opdelingen, bliver det også tydeligt at se, at B2 følger denne tendens langt bedre end B1.

\begin{figure}[H]
	\begin{minipage}[t]{0.6\linewidth}
		\includegraphics[width=7cm]{maximal_margin_hyperplane_1}
	\end{minipage}
	\begin{minipage}[t]{0.1\linewidth}
		\includegraphics[width=7cm]{maximal_margin_hyperplane_2}
	\end{minipage}
	\caption{\textit{Til venstre: Træningssæt. Til højre: Opdeling af sættet med hyperplaner.}}
\end{figure}


\subsection{Lineær SVM}
\subsubsection{Separerbare tilfælde}
I vores gennemgang af lineære SVM vil vi arbejde med bineær inddeling af datasæt. Vores inddeling kan beskrives mere formelt som $(x_i,y_i) \ i=1,2,...N$, hvor $x_i$ er et datapunkt og $y_i$ er punktets klasse. Af hensyn til udregningerne definerer vi $y_i \in \{-1,1\}$.

Hyperplanet, som udgør grænsen i en opdeling, beskriver vi med ligning (\ref{eq:hyperplane_equation}), hvor $w$ er en normalvektor til hyperplanet, $b$ er den vinkelrette afstand fra hyperplanet til origo, og $x$ er et punkt på hyperplanet.
\begin{equation}
\label{eq:hyperplane_equation}
w \cdot x + b = 0
\end{equation}

På figur \ref{fig:hyperplane_example} ses et eksempel på et plot af et datasæt. Hvis der lå punkter på $B1$, ville det opfylde betingelserne for $x$ i (\ref{eq:hyperplane_equation}).
\begin{center}
\begin{figure}[H]
\includegraphics[width=8cm]{svm_plot}
\caption{\textit{Separeret data}}
\label{fig:hyperplane_example}
\end{figure}
\end{center}


For at klassificere et punkt må vi afgøre, hvilken side af grænsen punktet befinder sig. Dette kan formaliseres med gaffelfunktionen (\ref{eq:point_classification}). 
\begin{equation}
\label{eq:point_classification}
y_i = \left\{
\begin{array}{lr}
1, & if \ w \cdot x_i + b > 0 \\
-1, & if \ w \cdot x_i + b < 0
\end{array}
\right.
\end{equation}
Punkter som opfylder $w\cdot x_i + b <0$ ligger under grænsen, og vil blive klassificeret som $-1$, mens punkter der opfylder $w\cdot x_i +b >0$ ligger over grænsen, og vil blive klassificeret som $1$. Hvis vi kigger på figur \ref{fig:hyperplane_example} vil cirklerne blive kvalificeret som $1$, og stjernerne blive klassificeret som $-1$.

Lad $x_1$ og $x_2$ være de punkter, der ligger nærmest hyperplanet, sådan at $x_1$ ligger over hyperplanet og $x_2$ under. Så ligger $x_1$ og $x_2$ på marginerne hhv. $b_{11}$ og $b_{12}$ (se figur \ref{fig:hyperplane_example}). Ved at skalere $w$ og $b$ kan kan vi beskrive marginerne som (\ref{eq:margin_1}) og (\ref{eq:margin_2}). Da vil (\ref{eq:margin_1}) være opfyldt for $x=x_1$ og (\ref{eq:margin_2}) for $x=x_2$.
\begin{equation}
\label{eq:margin_1}
b_{11}: w\cdot x + b = 1
\end{equation}
\begin{equation}
\label{eq:margin_2}
b_{12}: w\cdot x + b = -1
\end{equation}

Da målet er at maksimere margin, skal vi være i stand til at udregne afstanden $d$ mellem de to marginer $b_{11}$ og $b_{12}$. Ved at indsætte $x_1$ og $x_2$ i marginernes ligninger og trække dem fra hinanden kan vi netop udregne afstanden mellem dem. Vi observerer først
\begin{align}
(w\cdot x_1 + b) - (w\cdot x_2 +b) &= 1 - (-1)\\
w\cdot x_1 - w\cdot x_2 &= 2\\
w\cdot(x_1-x_2)&=2
\label{eq:d_1}
\end{align}
Bemærker at venstre side af (\ref{eq:d_1}) er prikproduktet mellem to euklediske vektorer. Vi kan da udnytte at $A\cdot B = ||A||\times||B||\times\cos\theta$, hvor $A$ og $B$ er euklediske vektorer, og $\theta$ er vinklen mellem dem:
\begin{equation}
||w||\times||x_1-x_2||\cos\theta=2
\end{equation}
Vi benytter nu at den skalare projektion af $A$ på $B$ er givet ved $A_B=||A||\cos\theta$, og at bredden $d$ netop er projektionen af $(x_1-x_2)$ på $w$. Vi får altså:
\begin{align}
||w||\times d &= 2\\
\label{eq:d_2}
d&=\frac{2}{||w||}
\end{align}


For at træne vores SVM skal vi maksimere $d$, sådan at betingelserne i (\ref{eq:req_1}) og (\ref{eq:req_2}) er opfyldt for alle $x_i$.
\begin{align}
\label{eq:req_1}
w \cdot x_i + b \geq 1 \text{ for } \ y_i = 1\\
\label{eq:req_2}
w \cdot x_i + b \leq -1 \text{ for } y_i = -1
\end{align}

Da fortegnet på $y_i$ vil svare til fortegnet af $w\cdot x_i+b$ og $||w\cdot x_i+b||\geq 1$ kan vi samle (\ref{eq:req_1}) og (\ref{eq:req_2}) i en enkel betingelse:
\begin{equation}
\label{eq:req_final}
y_i(w \cdot x_i + b) \geq 1, \ i =1,2,...,N
\end{equation}

Fra (\ref{eq:d_2}) er det klart, at for at maksimere $d$ må vi minimere $w$. Vi opstiller nu den objektive funktion i (\ref{eq:objective_function}).
\begin{equation}
\label{eq:objective_function}
f(w)=\frac{||w||^2}{2}
\end{equation}

Vi definerer nu optimeringen af det separerbare lineære SVM som en betinget minimeringen af $w$ i (\ref{eq:objective_function}) under betingelsen(\ref{eq:req_final}).



\subsubsection{Optimering}
For at optimere vores objektive funktion, skal vi bruge \textit{Lagrange multiplier}, \textit{Lagrange multiplier} er en metode der bruges til at finde en funktions minimum eller maksimum, og i dette tilfælde skal vi bruge den til at finde (\ref{eq:objective_function})'s minimum, da dette vil give os den bedste \textit{decision boundary}. Vi bruger \textit{Lagranges multiplier} da vores problem består af en kvadratisk objektiv funktion, og vores begrænsninger er lineære i parametrene $w$ og $b$, hvilket ender ud i at være et konvekst optimeringsproblem, og her bliver \textit{Langranges multiplier} brugt som standard.

Det ses let at i vores objektive funktion (\ref{eq:objective_function}), at for at opnå den minimale værdi ville $w = 0$ være den bedste løsning, men denne løsning ville ikke holde da vores betingelser i uligheden fra (\ref{eq:req_final}) ikke vil blive overholdt, og hvis uligheden ikke bliver overholdt, vil det ikke være muligt finde en løsning til problemet. For at løse dette problem, vil vi transformere vores objektive funktion \ref{eq:objective_function} til en \textit{Lagrangian} funktion, en \textit{Lagrangian} funktion inkorporerer vores ulighed fra (\ref{eq:req_final}) i vores objektive funktion (\ref{eq:objective_function}), og hvis uligheden ikke bliver overholdt, vil værdien af funktionen stige, og dermed give et dårligere resultat. Ved at have omskrevet vores objektive funktion (\ref{eq:objective_function}) til en \textit{Lagrangian} funktion, vil det også være muligt for os at bruge \textit{Lagrange multipliers}, og det ses også i funktionen (\ref{eq:L_P}), som er vores \textit{Lagragian} funktion hvor vores uligheder er blevet inkorporeret, at der er blevet indsat et $\lambda$, som er vores \textit{Lagrange multiplier}.
\begin{equation}
\label{eq:L_P}
L_P = \frac{1}{2}||w||^2 - \sum\limits_{i=1}^N \lambda_i (y_i(w \cdot x_i + b) - 1)
\end{equation}

For at kunne minimere vores \textit{Lagrangian} funktion, skal vi først lave to partielle differentieringer. Først differentierer vi i forhold til $w$, og derefter differentierer vi i forhold til $b$. Da vi ønsker at finde ekstremumspunkter, sættes de partielt differentierede til 0. Dette medfører resultatet nedenfor.
\begin{equation}
\label{eq:partial_L_pw}
\frac{\partial L_p}{\partial w} = 0 \Rightarrow w = \sum\limits_{i=1}^N \lambda_i y_i x_i
\end{equation}
\begin{equation}
\label{eq:partial_L_pb}
\frac{\partial L_p}{\partial b} = 0 \Rightarrow \sum\limits_{i=1}^N \lambda_i y_i = 0
\end{equation}
Selvom vi har fundet de partielt differentierede funktioner, er det stadig ikke muligt at finde $w$ og $b$, dette skyldes at vores definition af vores objektive funktion kun indeholder uligheder, og at \textit{Lagrange multiplier} skal bruge ligheder. Hvis vores definition havde indeholdt ligheder, ville det havde været muligt at finde værdierne til $w$, $b$ og $\lambda$ med de $N$ ligninger som vi har fundet. Men for at løse dette problem skal vi transformere vores uligheder til ligheder. Dette kan vi gøre med \textit{Karush-Kuhn-Tucker} betingelserne. Disse betingelser kan ses nedenfor.
\begin{equation}
\label{eq:constraint_lambda_i1}
\lambda_i \geq 0
\end{equation}
\begin{equation}
\label{eq:constraint_lambda_i2}
\lambda_i[y_i(w \cdot x_i + b) - 1] = 0
\end{equation}
Som det kan ses i vores nye ligheder, skal alle $\lambda_i$ være $0$ når $y_i(w \cdot x_i + b) \neq 0$, og ud fra dette ved vi at alle de punkter der ligger på $b_{i1}$ eller $b_{i2}$ har en \textit{Lagrange multiplier} $\lambda_i > 0$.  De punkter der ligger på $b_{i1}$ og $b_{i2}$ og opfylder betingelserne er også kendt som \textit{support vectors}.

Vores optimeringsproblem er stadig kompliceret grundet at vi har $w$, $b$ og $\lambda_i$. Vi kan simplificere problemet ved at bruge \textit{dual problem} metoden. Denne metode bruges ofte i sammenhæng med \textit{Lagrange mlutipliers}. Metoden transformerer vores \textit{Lagrangian} funktion til en funktion, som kun indeholder vores \textit{Lagrange multipliers}. Ved at indsætte konsekvenserne af de partielt differentierede sat til 0 i $L_P$ opnår vi vores \textit{dual problem} funktion som nedenfor.
\begin{equation}
\label{eq:L_D}
L_D = \sum\limits_{i=1}^N \lambda_i - \frac{1}{2}\sum\limits_{i,j} \lambda_i \lambda_j y_i y_j x_j \cdot x_j
\end{equation}
Der er to store forskelle på vores primære \textit{Lagrangian} funktion, og vores nye \textit{dual problem} funktion. Den første store forskel er at vores \textit{dual problem} kun indeholder vores trænings data samt vores \textit{Lagrange multipliers}, hvor vores primære \textit{Lagrangian} funktion også indeholder vores $w$ og $b$. Den anden store forskel er, at vores minimeringsproblem  er blevet til et maksimeringsproblem.
Løsningen af vores \textit{dual problem} kræver at vi finder værdierne af $\lambda_i$, hvilket opnås gennem $\textit{quadratic programming}$. Detaljerne ligger undertiden uden for emnet i denne rapport.\\
Efter at  vi har fundet $\lambda_i$ ved hjælp af \textit{quadratic programming} kan vi finde $w$ og $b$. Vi kan beskrive vores grænse med ligningen nedenfor.
\begin{equation}
\label{eq:final_opt}
( \sum\limits_{i=1}^N \lambda_i y_i x_i \cdot x) + b = 0
\end{equation}

Når vi endelig er færdig med at beregne alle vores parametre for vores grænse, kan vi stille en funktionen op, som kan ses nedenunder, den funktion kan beregne hvilken klassifisering et givet datapunkt $z$ tilhører. Hvis $f(z) = 1$ vil $z$ blive klassificeret som $1$ ellers vil $z$ blive kvalificeret som $-1$. 
\begin{equation}
\label{eq:calc_class}
f(z) = sign(w \cdot z + b) = sign( \sum\limits_{i=1}^N \lambda_i y_i x_i \cdot z + b)
\end{equation}

\subsubsection{Ikke-separerbare tilfælde}
I nogle tilfælde vil den bedste grænse (\textit{decision boundary}) ikke altid klassificere alle punkter i det givet dataset korrekt. Dette kan skyldes forskellige ting såsom fejl i træningsdata, eller at der er et enkelt dataset, som ikke holder sig til mønstret for dens klassificering. I disse tilfælde vil vi stadig prøve at opnå så stor en afstand mellem vores margener som muligt, selvom enkelte dataset bliver klassificeret forkert. Dette skyldes at hvis vi valgte en grænse, som ville opdele vores træningssæt uden fejl, ville grænsen ofte have en meget lille afstand mellems dens margener, og dette vil gøre at modellen ville være meget udsat for \textit{overfitting}. På grafen nedenfor ses et eksempel, hvor at $B2$ ville være den oplagte grænse at vælge, da den opdeler alt data korrekt, men vi også vælge $B1$ grundet at den har en større afstand mellem sine margener, og risikoen for \textit{overfitting} ville være mindre. 
\begin{figure}[H]
	\begin{minipage}[t]{0.6\linewidth}
		\includegraphics[width=7cm]{svm_plot_1}
	\end{minipage}
	\begin{minipage}[t]{0.1\linewidth}
		\includegraphics[width=7cm]{svm_plot_1_2}
	\end{minipage}
\caption{\textit{Til venstre: Opdeling af et ikke-sparerebart tilfælde. Til højre: Opdeling af et ikke-sparerebart tilfælde, med fejl kvalificeret punkter.}}
\label{fig:hyperplane_example_1}
\end{figure}
Grundet at vi har punkter der ikke er klassificeret rigtigt af vores grænse, vil vores tidligere ligninger ikke passe på alle vores dataset. For at rette dette introducerer vi \textit{slack variables} ($\xi$). Disse variabler skal estimere hvor stor afstand der er til det korrekte hyperplan, og dette gøres ved at addere eller subtrahere (hvilke der skal bruges kommer an på hvor datapunktet er placeret) med vores nye variable ($\xi$). 

 Som det kan ses i vores to nye ligninger skal vi subtrahere hvis vores dataset er placeret under grænsen, men den skal kvalificeres, som om den lå oven for grænsen, dette gør vi ved at subtraherer  højresiden af ligningen med vores \textit{slack variable}, med en værdi som gør at formlen er sand. Det samme gælder for det omvendte scenarie, men her bruges addition på højre siden af ligningen for at få ligningen til at blive sand.  
\begin{equation}
\label{eq:slack_hyperplane_1}
w \cdot x_i + b \geq 1 - \xi_i \ hvis \ y_i = 1
\end{equation}
\begin{equation}
\label{eq:slack_hyperplane_2}
w \cdot x_i + b \leq -1 +\xi_i \ hvis \ y_i = -1
\end{equation}
Bemærk $\forall i : \xi_i > 0$

For ikke bare at vælge en grænse med store margener, og så bare rette alle de forkerte klassificeret dataset til med \textit{slack variables}, introducerer vi en vægt for hvert datapunkt, som er kvalificeret forkert. Denne vægt kan ses i den modificerede objektive funktion nedenfor, bemærk at $C$ og $k$ bliver sat a brugeren (ofte vil $k = 1$ grundet simplicitet). Det ses klart at for hvert punkt, som er kvalificeret forkert, vil de valgte margener blive rangeret dårligere. 
\begin{equation}
\label{eq:slack_objective_function}
f(w) = \frac{||w||^2}{2}+C(\sum\limits_{i=1}^N \xi_i)^k
\end{equation}

\subsubsection{Optimering}
Optimering med vores \textit{slack variable} er næsten ens med vores optimering uden vores ekstra variable. Igen vil vi starte med at lave vores objektive funktion om til en \textit{Lagrangian} funktion.
\begin{equation}
\label{eq:slack_L_P}
L_P = \frac{1}{2}||w||^2 + C \sum\limits_{i=1}^N \xi_i - \sum\limits_{i=1}^N \lambda_i (y_i(w \cdot x_i + b) - 1 + \xi_i) - \sum\limits_{i=1}^N \mu_i \xi_i
\end{equation}
For at forstå funktionen er det nemmest hvis man deler den op. Funktionen kan deles op i fire dele, hvor de to første dele er vores objektive funktion, som vi prøver at minimere, den tredje del  repræsenterer vores uligheder, er den sidste og fjerde del er kravet om at $\xi$ ikke må være negativt. 

Igen skal vi transformere vores uligheder til ligheder, og her vil vi igen bruge \textit{Karush-Kuhn-Tucker's} metode til at transformere dem. Nedenfor kan vores nye betingelser findes.
\begin{equation}
\label{eq:slack_constraint_lambda_i1}
\xi_i \geq 0, \lambda_i \geq 0, \mu_i \geq 0
\end{equation}
\begin{equation}
\label{eq:slack_constraint_lambda_i2}
\lambda_i(y_i(w \cdot x_i + b) - 1 + \xi_i) = 0
\end{equation}
\begin{equation}
\label{eq:slack_constraint_lambda_i3}
\mu_i \xi_i = 0
\end{equation}

Igen finder vi de partielt differentieret funktioner af $L_p$.
\begin{equation}
\label{eq:slack_partial_L_pw}
\frac{\partial L_p}{\partial w_j} = w_j - \sum\limits_{i=1}^N \lambda_i y_i x_{ij} = 0 \Rightarrow w_j = \sum\limits_{i=1}^N \lambda_i y_i x_{ij}
\end{equation}
\begin{equation}
\label{eq:slack_partial_L_pb}
\frac{\partial L_p}{\partial b} = - \sum\limits_{i=1}^N \lambda_i y_i = 0 \Rightarrow \sum\limits_{i=1}^N \lambda_i y_i = 0
\end{equation}
\begin{equation}
\label{eq:slack_partial_L_pxi}
\frac{\partial L_p}{\partial \xi_i} = C - \lambda_i - \mu_i = 0 \Rightarrow \lambda_i + \mu_i = C
\end{equation}

Hvis man som sidst indsætter vores nye ligninger ind i vores \textit{Lagrangian} funktion får vi igen et \textit{dual problem}, og dette er det samme som vi fandt frem til i det \textit{separable} tilfaelde. Den skiller sig lidt ud da vi har lidt flere betingelser, sidst havde vi bare at $\lambda_i \geq 0$, her har vi at $C \geq \lambda_i \geq 0$. Herfra er det den samme fremgangsmåde med \textit{quadratic programming} etc, som i det \textit{separable} tilfælde, se (\ref{eq:final_opt}) og (\ref{eq:calc_class}).
\begin{equation}
\label{eq:slack_L_D}
L_D = \sum\limits_{i=1}^N \lambda_i - \frac{1}{2}\sum\limits_{i,j}\lambda_i \lambda_j y_i y_j x_i \cdot x_j
\end{equation}

\subsection{Ikke-lineær SVM}
Det er ikke altid muligt at definere en lineær opdeling af dataen. Den tilgængelige data er ej heller nødvendigvis på vektorform - dataen kan f.eks. bestå af grafer. Den naive løsning på dette problem er at transformere datapunkterne til et andet rum, hvor en lineær opdeling er mulig.

Antag at vi har en transformation $\Phi(x)$ fra det originale rum til et transformeret rum, hvor en lineær opdeling er mulig. Så kan vi genbruge vores viden fra det lineære tilfælde, og optimeringen er nogenlunde den samme. For at opnå den maksimale margin skal vi igen minimere funktionen 
$$f(w)=\frac{||w||^2}{2}$$
Under kravet at
$$y_i(w\cdot\Phi(x_i)+b)\geq 1,\text{ for }i=1,2..,N$$
Ligningen er næsten identisk med det lineære tilfælde, men vi optimerer nu med $\Phi(x_i)$ - altså $x_i$ i det transformerede rum.

Lagrange kan nu benyttes på samme måde som i det lineære tilfælde. Også her skal vi blot benytte de transfomerede punkter.

\begin{align*}
L_D&=\sum_{i=1}^{n}\lambda_i-\frac{1}{2}\sum_{i,j}\lambda_i\lambda_j y_iy_j\Phi(x_i)\cdot\Phi(x_j)
\end{align*}

Denne optimering kræver som man kan se udregningen af prikproduktet mellem samtlige par af punkter i det transformerede rum. Da dette kan være udregningsmæssigt tungt, og kræver at man kender en transformation der passer på det enkelte problem, vil man typisk bruge en anden tilgang kaldet \textit{kernel trick}, som tillader os at benytte kernefunktioner til at opnå et ækvivalent resultat.


\subsubsection{Kernefunktioner}
Lad $X=\{x_1..x_n\}\subset \chi_1$ være en mængde af $n$ vektorer i rummet $\chi_1$, og lad $\Phi:\chi_1\rightarrow\chi_2$. Da er en kernefunktion $k$ med feature mapping $\Phi$ en funktion der udregner det indre produkt af ethvert vektorpar $x_i,x_j\in X$ transformeret til $\chi_2$. Altså
\begin{equation}
k(x_i,x_j)=\langle\Phi(x_i),\Phi(x_j)\rangle
\end{equation}

Så definerer vi kernen $K$ som en $n\times n$-matrice med indre produkter af input vektorerne (en Gram matrice)
\begin{equation}
K=\left[\begin{array}{r r r}
\langle\Phi(x_1),\Phi(x_1)\rangle & ... & \langle\Phi(x_1),\Phi(x_n)\rangle\\
 & ... & \\
\langle\Phi(x_n),\Phi(x_1)\rangle & ... & \langle\Phi(x_n),\Phi(x_n)\rangle\\
\end{array}\right]\\
\end{equation}
Et indre produkt er per definition symmetrisk, så det følger at
\begin{equation}
K_{i,j}=\langle\Phi(x_i),\Phi(x_j)\rangle=\langle\Phi(x_j),\Phi(x_i)\rangle=K_{j,i}
\end{equation}
Hvilket viser at $K$ må være symmetrisk over diagonalen. 

Desuden vil en Gram matrice være positiv semidefinit. Dette kan bl.a. formaliseres under uligheden:
\begin{equation}
\forall c\in R^n,c^TKc\geq 0,\text{ for }c\neq 0
\end{equation}
Altså: for enhver gyldig kernematrice $K$ og en vilkårlig ikke-nulvektor $c$ vil produktet af $c^TKc$ være ikke-negativt. Det følger at kernefunktionen $k$ er positiv semi-definit, hvilket er en central egenskab ved kernefunktioner.

Med denne definition kan $k$ nu erstatte prikproduktet og transformationerne i dual Lagrange problemet fra tidligere:
\begin{align*}
L_D&=\sum_{i=1}^{n}\lambda_i-\frac{1}{2}\sum_{i,j}\lambda_i\lambda_j y_iy_jk(x_i,x_j)
\end{align*}


\subsubsection{Mercers teorem}
Lad $k:X\times X\rightarrow \mathbb{R}$ være en symmetrisk, positivt semidefinit funktion på $X={x_1,..x_n}$. Så ønsker vi at vise at der findes en transformation $\Phi:X\rightarrow H$ hvor $H$ er et Hilbert rum sådan at 
\begin{equation}
\label{eq:mercers}
\langle\phi(x_i),\phi(x_j)\rangle=k(x_i,x_j)
\end{equation}

Betragt matricen $K:$
\begin{equation}
K=\left[\begin{array}{r r r}
k(x_1,x_1) & ... &k(x_1,x_n)\\
& ... & \\
k(x_n,x_1) & ... &k(x_n,x_n)\\
\end{array}\right]\\
\end{equation}
Da $K$ er symmetrisk ved vi fra spektralsætningen at $K$ er diagonaliserbar med en ortogonal matrix $V$ sådan at $\Lambda=V^TKV$, hvor $\Lambda$ er en diagonalmatrix, hvor $\Lambda_{tt}$ er egenværdi $\lambda_t$ for $K$, og hvor kolonne $t$ i $V$ er den tilhørende egenvektorer $v_t$.

Betragt nu transformationen 
\begin{equation}
\Phi(x_i)\rightarrow\left(\sqrt{\lambda_t}v_{ti}\right)_{t=1}^n\\
\end{equation}
Så får vi
\begin{align}
\langle\Phi(x_i),\Phi(x_j)\rangle&=\left(\sqrt{\lambda_t}v_{ti}\right)_{t=1}^n\cdot\left(\sqrt{\lambda_t}v_{tj}\right)_{t=1}^n\\
&=\sum_{t=1}^n\sqrt{\lambda_t}v_{ti}\sqrt{\lambda_t}v_{tj}=\sum_{t=1}^{n}\lambda_tv_iv_j\\
&=K_{ij}=k(x_i,x_j)
\end{align}
Vi har nu vist at der for en symmetrisk funktion $k=(x_i,x_j)$ vil eksistere en transformation $\Phi$ sådan at (\ref{eq:mercers}) er opfyldt.



\section{Grafkerner}
I dette afsnit vil vi kigge specifikt på kernefunktioner til sammenligning af grafer. Vi starter med et eksempel på en etableret grafkerne, og introducerer efterfølgende eksempler på en række simple grafkerner baseret på \cite{trivial-kernels}. Vi kommer herefter ind på krydsvalidering ved estimering af en models nøjagtighed, og præsenterer til sidst resultaterne af vores eksperimenter, hvor vi sammenligner præstationen af de uortodokse kerner overfor de etablerede kerner samt de postulerede resultater fra \cite{trivial-kernels}.

\subsection{Shortest-path}
Shortest-path kernen er en veletableret kerne til sammenligning af grafer. Den præsenteres her på baggrund af definitionen i \cite{shortest-path}. 

Lad $G_1=(V_1,E_1)$ og $G_2=(V_2,E_2)$ være to grafer, som vi ønsker at sammenligne, hvor $\{v_{i1},v_{i2}..,v_{in}\}\in V_i$ og $\{e_{i1},e_{i2}..,e_{im}\}\in E_i$ er knuder hhv. kanter i $G_i$.

Det første skridt i algoritmen er at transformere graferne $G_1,G_2$ til shortest-path grafer $S_1,S_2$. Vi definerer en shortest-path graf således:

\textit{Lad $S=(V,E')$ være shortest-path grafen for $G=(V,E)$, og lad $v_i,v_j\in V$. Så findes der en kant $e_{ij}=(v_i,v_j)\in E'$ hvis og kun hvis der findes en sti $\pi_{ij}$ i $G$ fra $v_i$ til $v_j$, og længden af $e_{ij}$ vil være længden af den korteste sti $\pi_{ij}'$}

I figur \ref{fig:shortest-path-graph} illustreres transformationen fra en graf $G$ til dens shortest-path graf $S$.

\begin{figure}[H]
\begin{minipage}[t]{0.7\linewidth}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]

\node[main node] (a) [label=a] {};
\node[main node] (b) [below of=a, label=b] {};
\node[main node] (c) [right of=a, below of=a, label=c] {};
\node[main node] (d) [right of=a, label=d] {};


\path[every node/.style={font=\sffamily\small}]
(a) edge node {5} (d)
(a) edge [bend right] node {2}(b)
(b) edge node {12} (d)
(d) edge [bend right] node{-1}(c)
(b) edge [bend right] node{1}(a)
;
\end{tikzpicture}
\end{minipage}
\begin{minipage}[t]{0.2\linewidth}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4cm,
	thick,main node/.style={circle,fill=blue!20,draw,font=\sffamily\Large\bfseries}]
	
	\node[main node] (a) [label=a] {};
	\node[main node] (b) [below of=a, label=b] {};
	\node[main node] (c) [right of=a, below of=a, label=c] {};
	\node[main node] (d) [right of=a, label=d] {};
	
	
	
	\path[every node/.style={font=\sffamily\small}]
	(a) edge node {5} (d)
	(a) edge [bend right] node {2}(b)
	(a) edge [bend right] node {4} (c)
	(b) edge node {5} (c)
	(b) edge [bend right] node {1} (a)
	(b) edge [bend right] node {6} (d)
	(d) edge [bend right] node {-1} (c)
	;
	\end{tikzpicture}
\end{minipage}
\caption{\textit{Til venstre: En graf G. Til højre: shortest-path grafen S for G.}}
\label{fig:shortest-path-graph}
\end{figure}

Denne transformation er et \textit{all pairs shortest path} problem, som kan løses med flere algoritmer. Artikel \cite{shortest-path} foreslår Floyd-transformation ved hjælp af Floyd-Warshall algoritmen.

Med de to transformerede grafer $S_1=(V_1,E_1')$ og $S_2=(V_2,E_2')$ defineres kernen nu således:
\begin{equation}
k_{shortest-paths}(S_1,S_2)=\sum_{e_1\in E_1'}\sum_{e_2\in E_2'}k^{(1)}_{walk}(e_1,e_2)
\end{equation}
hvor $k^{(1)}_{walk}$ er en kerne på \textit{edge walks} af længde 1. I praksis sammenligner algoritmen hvert par af kanter $e_1\in E_1',\ e_2\in E_2'$ og summerer deres similaritet til en total similaritet af graferne. Sammenligningen af kanterne sker gennem en $k^{(1)}_{walk}$ kerne. En sådan kerne kan sammenligne kanterne samt deres endepunkter. Dette kunne være på formen:
\begin{equation}
k^{(1)}_{walk}(e_1,e_2) = k_{edge}(e_1,e_2)k_{node}(v_{11},v_{21})k_{node}(v_{21},v_{22})
\end{equation}
hvor $e_1=(v_{11},v_{12}),e_2=(v_{21},v_{22})$. Hvis vi antager at grafernes knuder har diskrete værdier, kunne man f.eks. lade $k_{edge}$ være en Gaussisk kerne (\ref{eq:k_edge}) der sammenligner længden af kanterne, og $k_{node}$ være en binær kerne (\ref{eq:k_node}), som kræver at endepunkterne er ens. Lad $len(e)$ angive længden af en kant $e$ og $attr(v)$ angive en knudes (diskrete) egenskaber. Så kunne man opstille kernerne:
\begin{align}
\label{eq:k_edge}
k_{edge}(e_1,e_2)&=exp\left(-\frac{(length(e_1)-length(e_2))^2}{\eps}\right)\\
\label{eq:k_node}
k_{node}(v_1,v_2)&=
\begin{cases}
1 & \text{for }attr(v_1)=attr(v_2)\\
0 & \text{ellers}
\end{cases}
\end{align}
hvor $\eps$ er en parameter i træningsmodellen. Hele denne $k_{walk}^{(1)}$ kerne giver altså en sammenligning af kanternes længde, hvis deres endepunkter er identiske, og 0 ellers.

\subsection{Simple grafkerner}
Vi vil her gennemgå tre simple grafkerner, som vi vil inddrage i vores eksperimenter. Heraf er to af kernerne ($baseline_1,baseline_2$) baseret direkte på definitionerne i \cite{trivial-kernels}.

\subsubsection{$baseline_1$}
Denne kerne fokuserer udelukkende på egenskaberne i grafernes knuder, og ignorerer som sådan deres interne struktur. Dette opnås ved at bygge et histogram over hver grafs knudeegenskaber, og sammenligne de to histogrammer i en Gaussisk kerne.\\
Lad $G_1$ og $G_2$ være to grafer, hvor hver knude har en diskret egenskab. Lad da $l(G_1)$ og $l(G_2)$ være histogrammer for egenskaberne af knuderne i $G_1$ hhv. $G_2$. Så kan vi beskrive kernefunktionen i (\ref{eq:k_baseline_1}), hvor $\epsilon$ er en parameter i modellen, som vælges med krydsvalidering på træningssættet.
\begin{equation}
\label{eq:k_baseline_1}
k_{baseline_1}(G_1,G_2)=exp\left(-\frac{\left(l(G_1)- l(G_2)\right)\cdot\left(l(G_1)- l(G_2)\right)}{\epsilon}\right)
\end{equation}


\subsubsection{$baseline_2$}
Vores $baseline_2$ er baseret paa ?? $baseline_2$, den gaussiske kerne vi bruger, er beskrevet ??. $Baseline_2$ er den simpelste grafkerne vi bruger, den sammenligner de givets grafer maengde af noder i en gaussisk kerne, graffernes labels bruges ikke. Inden at vi kan sammenligne grafferne, skal vi transformere dem om til en vektor, som indeholder maengden af graffernes noder, i figur ?? kan der ses hvor simpel denne transformation er.

\begin{figure}[H]
	\begin{minipage}{0.7\linewidth}
		\begin{algorithmic}[1]
			\Function{$baseline_2$Transformation}{$graphs$}
				\State vector = []
				\For{$i = 1 \textrm{ to } size(graphs)$}
					\State vector[i] = size(graphs[i])
				\EndFor
				\State \Return{vector}
			\EndFunction
		\end{algorithmic}
	\end{minipage}
\caption{\textit{Pseudokode af transformationen i $baseline_2$}}
\label{fig:baseline_2}
\end{figure}

\subsubsection{$degree\ distributions$}
Hvor $baseline_1$ kun benytter egenskaberne af knuderne, baseres denne kerne udelukkende  på den interne struktur i graferne. I vores implementation sammenligner kernen grafernes $degree\ distributions$ (gradfordelinger) i en Gaussisk kerne.

Lad $G_j=(V_j,E_j)$ være en graf. Lad da $nb(v)$ være mængden af naboknuder til knuden $v\in V_j$. Så definerer vi graden af $v$
\begin{equation}
d(v) = |nb(v)|
\end{equation}
Da er gradfordelingen for $G_j$, $dd(G_j)$, en $m$-vektor, hvor
\begin{align}
m&=max(d(v) | v\in V_j)\\
dd(G_j)_h&=|(v | v\in V_j \land d(v) = h)|
\end{align}

Med denne definition af gradfordelinger kan vi sammenligne to grafer $G_1=(V_1,E_1),G_2=(V_2,E_2)$ med den Gaussiske kerne i (\ref{eq:k_degree_distributions}), hvor $\epsilon$ vælges med krydsvalidering.
\begin{equation}
\label{eq:k_degree_distributions}
k_{degree\_distributions}(G_1,G_2)=exp\left(-\frac{(dd(G_1)-dd(G_2))\cdot(dd(G_i)-dd(G_2))}{\epsilon}\right)
\end{equation}


\subsection{Krydsvalidering}
Vi så tidligere i afsnittet om ikke-separerbare tilfælde en objektiv formel (\ref{eq:slack_objective_function},\ref{eq:slack_objective_function_cross_validation}). Parameteren C i denne formel bestemmer, hvor meget miskvalificerede punkter straffes. 
\begin{equation}
f(w) = \frac{||w||^2}{2}+C(\sum\limits_{i=1}^N \xi_i)^k
\label{eq:slack_objective_function_cross_validation}
\end{equation}
For at optimere $C$ kan man træne sin SVM med en mængde af mulige $C$-værdier. Hvis man ønsker at forudsige modellens nøjagtighed på endnu ukendte værdier kan man dog ikke blot optimere på hele inputsættet, da man udsætter modellen for \textit{overfitting}. For at komme denne faldgruppe til livs benyttes krydsvalidering til at lave en realistisk forudsigelse af nøjagtigheden. Krydsvalidering kan også bruges til at optimere andre parametre i modellen - f.eks. benytter vi krydsvalidering til at optimere $\sigma$ i en Gaussisk kerne i vores eksperimenter med $baseline_1$ og $baseline_2$. I dette afsnit vil vi gennemgå $\textit{k-fold cross validation}$.

\textit{k-fold cross validation} er en \textit{non-exhaustive} krydsvalideringsmetode. Egenskaben \textit{non-exhaustive} betyder at valideringen ikke altid tester alle mulige delinger af inputsættet.\\
$k$ angiver hvor mange dele inputsættet deles op i samt antallet af iterationer i algoritmen. I hver iterationer benyttes en $k$'te del som testsæt, en $k$'te del som valideringssæt og de resterende $k-2$ dele som træningssæt. For et inputsæt $M$ i $k$ dele $M_1..M_k$ hvor $x\in M_i \land x\in M_j \Rightarrow i=j$ og $|M_i|=|M_j|=|M|/k$ samt en given iteration $i=1,2,..k$ kan man f.eks. vælge et testsæt:
\begin{equation}
\label{eq:cv_test}
Test_i = M_{k+1-1}
\end{equation}
Et valideringssæt:
\begin{equation}
\label{eq:cv_val}
Val_i = M_{k-(i\ mod\ k)}
\end{equation}
Vi får da et træningssæt:
\begin{equation}
Train_i = M\setminus \left(Test_i\cup Val_i\right)
\end{equation}

Vi vil her gennemgå \textit{10-fold cross-validation} til optimering af $C$ baseret på implementationen i \cite{libsvm}.\\
Foruden parameteren $k=10$ benytter algoritmen en endelig mængde $M_C=\{C_1,C_2,..C_m\}$ af mulige værdier for $C$, som vi ønsker at teste for. I hver iteration $i=1,2,..k$ af krydsvalideringen træner vi på $Train_i$ med hvert $C_j\in M_C$, og tester på $Val_i$. Vi vælger da det $C$ som gav den højeste nøjagtighed på valideringssættet, og træner nu på $Train_i\cup Val_i$ med dette $C$, hvorefter i vi tester på $Test_i$, og opnår en nøjagtighed $acc_i$. Efter sidste iteration udregnes gennemsnittet af $acc_1,acc_2,..acc_k$, hvilket vil være det endelige estimering af modellens nøjagtighed.

I figur \ref{fig:cross_validation} (venstre) illustreres hvordan testsættet (gråt) og valideringssættet (rødt) bliver rykket i hver iteration efter formlerne i \ref{eq:cv_test} hhv. \ref{eq:cv_val} for $k=10$. I figur \ref{fig:cross_validation} (højre) ses pseudokode for \textit{10-fold cross validation}. \\

\begin{figure}[H]
	\begin{minipage}{0.5\linewidth}
		\includegraphics[width=6cm]{10-fold-cross-validation_3}
	\end{minipage}
	\begin{minipage}{0.7\linewidth}
		\begin{algorithmic}[1]
			\Function{10-fold Cross Validation}{$K, lk$}
				\State k = 10
				\State $C$-values = [...]
				\State result = []
				\For{$i = 1 \textrm{ to } k$}
					\State validationSet = movePointerAndGetValidationSet(K)
					\State testSet = movePointerAndGetTestSet(K)
					\State imResult = []
					\For{$j = 1 \textrm{ to } size($C$-values)$}
						\State model = svmtrain((K - validationSet) - testSet, (lk - validationSet) - testSet , $C$-values[i])
						\State imResult[i] = svmpredict(mode, testSet).getAccuracy()
					\EndFor
					\State optimalC = $C$values[max(imResult).getIndex()]
					\State model = svmtrain(K - testSet, lk - testSet, optimalC)
					\State result[k] = svmpredict(model, testSet).getAccuracy()
				\EndFor
				\State \Return{mean(result)}
			\EndFunction
		\end{algorithmic}
	\end{minipage}
\caption{\textit{Til venstre: De forksellige iterationer i 10-fold cross validation. Til højre: Simplificeret pseudokode af libsvm's 10-fold cross validation.}}
\label{fig:cross_validation}
\end{figure}


\subsection{Eksperimenter}
For at verificere resultaterne i \cite{trivial-kernels} har vi selv gennemført eksperimenterne.
Vores primære mål med eksperimenterne er at verificere, at de to simple grafkerner beskrevet i \cite{trivial-kernels} kan måle sig med de etablerede og mere komplekse grafkerner. Til at tjekke dette har vi brugt fem forskellige datasaet (MUTAG, ENZYMES, DD, NCI1, NCI109). For at estimere grafkernernes nøjagtighed har vi brugt \textit{10-fold cross-validation} gennem \cite{libsvm} som beskrevet i afsnittet om krydsvalidering. 

\begin{table}[H]
\begin{center}
\scalebox{0.7} {
\begin{tabular}{c|c|c|c|c|c}
Metoder/Dataset & MUTAG & ENZYMES & DD & NCI1 & NCI109 \\
\hline
Baseline$_1$ & \boldmath$81.9444(\pm 2.0992)$ & $39.3500(\pm 1.2459)$ & $77.4786(\pm 0.6140)$ & $68.8200(\pm 0.7369)$ & $68.7427(\pm 0.3957)$ \\
Baseline$_2$ & $81.5556 (\pm 1.1355)$ & $21.3000 (\pm 1.7964)$ & $75.1538 (\pm 0.7320)$ & $62.3771 (\pm 0.6772)$ & $62.3956 (\pm 0.4050)$ \\
Degree Distributions &\boldmath$83.9444 (\pm 2.6248)$ &  $32.9833 (\pm 1.4582)$ & $74.8974 (\pm 0.6979)$ & $66.2238 (\pm 0.4372)$ & $65.2549 (\pm 0.5032)$ \\
Weisfeiler-Lehman & \boldmath$83.5000 (\pm 3.1322)$ & \boldmath$49.0667 (\pm 1.2229)$ & \boldmath$78.6923 (\pm 0.3273)$ & \boldmath$85.0097 (\pm 0.1907)$ & \boldmath$85.1845 (\pm 0.2438)$ \\
Ramon \& Gartner & \boldmath$83.0556 (\pm 1.8002)$ & - & - & - & - \\
GraphHopper & \boldmath$83.9444 (\pm 1.8040)$ & $36.2833 (\pm 1.5356)$ & - & $72.5985 (\pm 0.4184)$ & $71.4029 (\pm 0.3245)$ \\
Graphlet count & $74.1667 (\pm 2.2106)$ & $32.2000 (\pm 1.3896)$ & \boldmath$79.2735 (\pm 0.4789)$ & $65.9538 (\pm 0.2965)$ & $66.6383 (\pm 0.3391)$ \\
Random walk & $85.1667 (\pm 1.9075)$ & $36.2833 (\pm 1.5357)$ & - & - & - \\
Shortest path & \boldmath$85.6667 (\pm 2.2951)$ & $41.5500 (\pm 1.5457)$ & - & $73.1946 (\pm 0.3625)$ & $73.1311 (\pm 0.2525)$ \\
\end{tabular}
}
\caption{\textit{Resultater}}

\end{center}
\end{table}


\subsubsection{Kommentarer til resultater}

\section{Konklusion}



\renewcommand\refname{Referencer}
\begin{thebibliography}{9}
	\bibitem{shortest-path}
	Karsten M. Borgwardt, Hans-Peter Kriegel, 
	\emph{Shortest-path kernels on graphs},
	Institute for Computer Science, Ludwig-Maximilians-University Munich, 2005
	\bibitem{trivial-kernels}
	Yuliia Orlova, Morteza Alamgir, Ulrike von Luxburg, \textit{Graph kernel benchmark data sets are trivial!}, Department of Computer Science, University of Hamburg, 2015
	\bibitem{libsvm}
	(Software) Chih-Chung Chang, Chih-Jen Lin, \textit{LIBSVM v3.21}, 2015\\ \verb|https://www.csie.ntu.edu.tw/~cjlin/libsvm/text|
\end{thebibliography}


\end{document}